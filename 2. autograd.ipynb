{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUXD62zR2l_y"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1\n",
        "\n",
        "### $$f(X) =  \\frac{\\|X\\|_2^2}{n}$$, where $X\\in \\mathbb{R}^n$.\n",
        "### Find the gradient $f_X$, which is analytically $\\frac{2x}{n}$.\n",
        "\n",
        "\n",
        "### Generally, there are two ways:\n",
        "\n",
        "\n",
        "1.   backward()\n",
        "2.   autograd.grad()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2KgXqOgp3g7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n = 10\n",
        "X = torch.randn(10,requires_grad=True)\n",
        "Y = torch.square(X) # the same as X**2\n",
        "Ym = torch.mean(Y)\n",
        "print(X)\n",
        "print(Y)\n",
        "print(Ym)"
      ],
      "metadata": {
        "id": "62bCfGUC7uxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d8e79f-0236-403c-a208-1a5d48c48524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.2156, -0.2405, -0.1481, -0.0384, -0.5599,  0.8631,  0.1970,  0.9722,\n",
            "        -0.0123, -0.2951], requires_grad=True)\n",
            "tensor([4.9088e+00, 5.7862e-02, 2.1948e-02, 1.4720e-03, 3.1347e-01, 7.4499e-01,\n",
            "        3.8795e-02, 9.4508e-01, 1.5204e-04, 8.7070e-02],\n",
            "       grad_fn=<PowBackward0>)\n",
            "tensor(0.7120, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. backward()"
      ],
      "metadata": {
        "id": "mDMHtMcTRKSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ym.backward(retain_graph=True)\n",
        "print(X.grad)\n",
        "X.grad.zero_()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea2iu9OxRAy4",
        "outputId": "bd45474b-bf4c-44fc-ec15-e0ccceaf7c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.4431, -0.0481, -0.0296, -0.0077, -0.1120,  0.1726,  0.0394,  0.1944,\n",
            "        -0.0025, -0.0590])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. torch.autograd.grad()"
      ],
      "metadata": {
        "id": "Gs8IlGObTRQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_grad = torch.autograd.grad(Ym,X,create_graph = True,retain_graph = True)[0]\n",
        "print(X_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzn54EuAR8Fe",
        "outputId": "5083c255-47d0-4949-b458-1f85ef609074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.4431, -0.0481, -0.0296, -0.0077, -0.1120,  0.1726,  0.0394,  0.1944,\n",
            "        -0.0025, -0.0590], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `retain_graph = True` allows us to keep the hidden computational graph from deleting by `backward()` so that we can run this line again and again.\n",
        "\n",
        "* `create_graph = True` is used when we want to do further operations on gradients, so that the autograd engine can create a backpropable graph for operations done on gradients.\n",
        "\n",
        "* `grad.zero_()` avoid us to calculate the gradient accumulatively when we run this block again\n",
        "\n",
        "* `torch.autograd.grad()` does not have issue of accumulative gradient\n"
      ],
      "metadata": {
        "id": "Qwv7UXm2TZyP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P02Wj-r4SwVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "\n",
        "### $$F(A,X) = AX$$, where $A\\in\\mathbb{R}^{m\\times n}$ and $X\\in\\mathbb{R}^{n\\times p}$\n",
        "\n",
        "### Find the gradient $F_A$ and $F_X$\n",
        "\n",
        "### By Matrix Calculus, the gradient should be in form of tensor, but what pytorch is calculating is acutally the gradient of $J = (\\sum F_{ij})$.\n",
        "\n",
        "### $J_A$ and $J_X$ has the same shape as $A$ and $X$, respectively, and we have $$J = \\sum_{i,j}F_{ij} = \\sum_{i,j}\\sum_{k}A_{ik}X_{kj}$$\n",
        "\n",
        "### Hence, we would expect that $$(J_A)_{ql} = \\dfrac{dJ}{dA_{ql}} = \\sum_{i,j}\\sum_{k}\\dfrac{dA_{ik}}{dA_{ql}}X_{kj} = \\sum_j X_{lj}$$, which is the sum of $l$-th row of $X$ and is independent of $q$, so for each column of $J_A$, the components are the same.\n",
        "\n",
        "### Similarly, $$(J_X)_{ql} = \\dfrac{dJ}{dX_{ql}} = \\sum_{i,j}\\sum_{k}A_{ik}\\dfrac{dX_{kj}}{dX_{ql}} = \\sum_i A_{iq}$$, which is the sum of $q$-th column of $A$ and is independent of $l$, so for each row of $J_X$, the components are the same."
      ],
      "metadata": {
        "id": "BDbazOLGUK9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.randn(4,3,requires_grad=True)\n",
        "X = torch.randn(3,2,requires_grad=True)\n",
        "F = torch.matmul(A,X) # the same as A@X\n",
        "\n",
        "print(A)\n",
        "print(X)\n",
        "print(F)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruu-0aNkUbbI",
        "outputId": "fcebe818-eebe-4f6d-bb43-5a53844fc661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.4157, -0.5494, -0.3957],\n",
            "        [-0.2589,  0.4123,  0.0835],\n",
            "        [-1.2110,  1.4903, -1.2146],\n",
            "        [ 0.2153,  1.4981, -0.0765]], requires_grad=True)\n",
            "tensor([[ 1.3992,  1.1037],\n",
            "        [ 1.5777, -0.2932],\n",
            "        [ 0.6247, -0.8932]], requires_grad=True)\n",
            "tensor([[-3.0949, -1.0479],\n",
            "        [ 0.3404, -0.4812],\n",
            "        [-0.1021, -0.6886],\n",
            "        [ 2.6170, -0.1333]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. backward()"
      ],
      "metadata": {
        "id": "b2J-FUL6U6mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F.backward(torch.ones_like(F),retain_graph=True)\n",
        "\n",
        "print(A.grad)\n",
        "print(X.grad)\n",
        "A.grad.zero_()\n",
        "X.grad.zero_()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghkzdNQVUkBf",
        "outputId": "6c2aae91-a1f6-45ba-c853-a7af6cbd33d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.5029,  1.2845, -0.2685],\n",
            "        [ 2.5029,  1.2845, -0.2685],\n",
            "        [ 2.5029,  1.2845, -0.2685],\n",
            "        [ 2.5029,  1.2845, -0.2685]])\n",
            "tensor([[-2.6703, -2.6703],\n",
            "        [ 2.8512,  2.8512],\n",
            "        [-1.6033, -1.6033]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. autograd.grad()"
      ],
      "metadata": {
        "id": "WUoaEGYaWvz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A_grad = torch.autograd.grad(F,A, grad_outputs= torch.ones_like(F),create_graph= True, retain_graph = True)[0]\n",
        "X_grad = torch.autograd.grad(F,X, grad_outputs= torch.ones_like(F),create_graph= True, retain_graph = True)[0]\n",
        "\n",
        "print(A_grad)\n",
        "print(X_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4p2vyJoWimJ",
        "outputId": "47979108-f154-442a-898b-137a3460d083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.5029,  1.2845, -0.2685],\n",
            "        [ 2.5029,  1.2845, -0.2685],\n",
            "        [ 2.5029,  1.2845, -0.2685],\n",
            "        [ 2.5029,  1.2845, -0.2685]], grad_fn=<MmBackward0>)\n",
            "tensor([[-2.6703, -2.6703],\n",
            "        [ 2.8512,  2.8512],\n",
            "        [-1.6033, -1.6033]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* eventhough both functions are calculating the gradient of $J$, a scalar, our $F$ is defined as matrix, so we need to specify that by using `torch.ones_like(F)`, otherwise, error will be returned.\n",
        "\n",
        "* we will get the same result if use `F.sum()` to replace `F`"
      ],
      "metadata": {
        "id": "py-TPpNtho2T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10B3zeGKaBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3\n",
        "\n",
        "###$$f(x) = x^3$$\n",
        "\n",
        "### find the second derivartive of this function"
      ],
      "metadata": {
        "id": "JmrkUE71jO40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0,requires_grad=True)\n",
        "f = x**3\n",
        "\n",
        "print(x)\n",
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0TgIo_-jPtm",
        "outputId": "f6ab893b-482e-49b3-e3c6-75c7bbaa61c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(27., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. backward()"
      ],
      "metadata": {
        "id": "OQK3SmMslfw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f.backward(create_graph=True,retain_graph=True)\n",
        "f_prime = x.grad * torch.tensor(1.0)\n",
        "x.grad.zero_()\n",
        "f_prime.backward(retain_graph=True)\n",
        "print(x.grad)\n",
        "x.grad.zero_()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8JC6Pe8lVnz",
        "outputId": "530718ce-7de0-45b3-c5ae-6619d2404eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18., grad_fn=<ZeroBackward0>)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1177.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* if `f_prime = x.grad`, then the result is the addition of the first detivative and the second derivative.\n",
        "  * `f_prime` is **sharing** value with `x.grad`, so when we use `x.grad.zero_()` to avoid the addition of two derivaitves, the value of `f_prime` is also changed to zero, which is not wanted.\n",
        "\n",
        "\n",
        "* This multiplication with `torch.tensor(1.0)` does not change the value but establishes a computational graph that connects the `f_prime` with the first derivative, and `f_prime` is not sharing value with `x.grad` anymore.\n",
        "\n"
      ],
      "metadata": {
        "id": "FBSDqz3Osgr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. autograd.grad()"
      ],
      "metadata": {
        "id": "xxm5ZGDqnXzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_prime = torch.autograd.grad(f,x,create_graph= True, retain_graph = True)[0]\n",
        "f_prime_prime = torch.autograd.grad(f_prime,x,retain_graph=True)[0]\n",
        "print(f_prime_prime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVtvZA9Mm0Wp",
        "outputId": "5df37545-d21c-4fbb-f717-5b9393faf8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wmj6SoeqmMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5, requires_grad=True)\n",
        "h = torch.exp(x).sum()\n",
        "h.backward()  # First backward pass\n",
        "x.grad.zero_()  # Reset gradients to zero\n",
        "x.grad += torch.ones_like(x)  # Manually setting gradients\n",
        "h = torch.exp(x).sum()\n",
        "h.backward()  # Second backward pass\n",
        "print(x.grad)  # Gradient afte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHDH-Heaw0LP",
        "outputId": "fd34c14f-8b9a-4ae5-b815-420772a5b1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.3913, 2.0521, 2.1245, 2.3662, 2.8886])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.exp(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KPE2zIIw0ms",
        "outputId": "f99956f9-3540-43c2-f35e-e23c946d918d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.3913, 1.0521, 1.1245, 1.3662, 1.8886], grad_fn=<ExpBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkRnISdFxDmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}